{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis - Data Scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for individual webpages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for scariping Boligsiden.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get URL for Boligsiden search for specified period in selected Kommune\n",
    "\n",
    "def get_url_boligsiden(kommune, startdate, enddate, p):\n",
    "    url = 'http://www.boligsiden.dk/salgspris/solgt/alle/{}'\n",
    "    params = '?periode.from={}&periode.to={}&displaytab=mergedtab&sort' \\\n",
    "             '=salgsdato&salgstype=%5Bobject%20Object%5D&kommune={}'\n",
    "    full_url = url + params\n",
    "    return full_url.format(p, startdate, enddate, kommune)\n",
    "\n",
    "#### Get number of pages for Boligsiden search\n",
    "\n",
    "def get_max_pages_boligsiden(url):\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    driver = webdriver.Chrome('/Users/Mikkel/Documents/Drivers/chromedriver', options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    page_text = driver.find_element_by_class_name(\"salesprice-result\").text\n",
    "\n",
    "    last_page_num = (page_text.split(\"af \")[1]).split(\"\\n\")[0]\n",
    "    return last_page_num\n",
    "\n",
    "#### Get all address links on search page\n",
    "\n",
    "def get_all_urls_on_page_boligsiden(url):\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    driver = webdriver.Chrome('/Users/Mikkel/Documents/Drivers/chromedriver', options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    all_https = []\n",
    "    with_reentries_https = []\n",
    "\n",
    "    for elem in driver.find_elements_by_tag_name('a'):\n",
    "        all_https.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "    #bolig-links wanted appear multiple times, so we take away all single time occuring links\n",
    "    for i in range(len(all_https)):\n",
    "        if all_https[i] in all_https[:i]:\n",
    "            with_reentries_https.append(all_https[i])\n",
    "\n",
    "    #Take away first two entries, which are not bolig links\n",
    "    with_reentries_https = with_reentries_https[2:]\n",
    "\n",
    "    reduced_list = list(set(with_reentries_https))\n",
    "\n",
    "    #To make sure no other links are included\n",
    "    boliger_https = []\n",
    "    condition = 'https://www.boligsiden.dk/adresse/'\n",
    "    entry = 0\n",
    "    error_count = 0\n",
    "    for i in reduced_list:\n",
    "        if isinstance(i, str):\n",
    "            if condition in i:\n",
    "                boliger_https.append(i)\n",
    "\n",
    "    return boliger_https\n",
    "\n",
    "#### Get list of all address URLs for search\n",
    "\n",
    "def get_all_links_boligsiden(kommune, startdate, enddate):\n",
    "    # Returns first https-page with given variables\n",
    "    first_page = get_url_boligsiden(kommune, startdate, enddate, 1)\n",
    "\n",
    "    # Getting number of total pages\n",
    "    total_pages = get_max_pages_boligsiden(first_page)\n",
    "\n",
    "    # Empty lists\n",
    "    link_to_all_pages = []\n",
    "    list_of_all_pages = []\n",
    "\n",
    "    # Collects a list with all the pages that we want to collect\n",
    "    for x in tqdm(range(int(total_pages))):\n",
    "        all_pages = get_url_boligsiden(kommune, startdate, enddate, x + 1)\n",
    "        link_to_all_pages.append(all_pages)\n",
    "\n",
    "        page_list = get_all_urls_on_page_boligsiden(link_to_all_pages[x])\n",
    "        list_of_all_pages.extend(page_list)\n",
    "\n",
    "    # Returns list with all the wanted url's\n",
    "    return (list_of_all_pages)\n",
    "\n",
    "#### Scrape information for single address on address URL \n",
    "\n",
    "def get_simple_single_page_boligsiden(url):\n",
    "\n",
    "    url = url\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "    head = str(soup.find('head'))\n",
    "    try:\n",
    "        json_string = re.search(r'__bs_addresspresentation__ = ([^;]*)', head).group(1)\n",
    "        data = json.loads(json_string)\n",
    "        df1 = pd.json_normalize(data)\n",
    "        df2 = pd.DataFrame()\n",
    "    except:\n",
    "        json_string = re.search(r'__bs_propertypresentation__ = ([^;]*)', head).group(1)\n",
    "        data = json.loads(json_string)\n",
    "        df2 = pd.json_normalize(data)\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "#### Collect scraped information for all addresses in two dataframes\n",
    "\n",
    "def get_data_boligsiden(links):\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "\n",
    "    for x in tqdm(range(0, len(links))):\n",
    "        try:\n",
    "            df_pages1, df_pages2 = get_simple_single_page_boligsiden(links[x])\n",
    "            df1 = pd.concat([df1, df_pages1])\n",
    "            df2 = pd.concat([df2, df_pages2])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for scraping DinGeo.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get DinGeo-URLs for all addresses in Boligsiden dataframes \n",
    "\n",
    "def get_geolinks1(df):\n",
    "    df[\"dingeo_link\"] = \"\"\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if '-' in (df['address.street'][x]):\n",
    "            df['address.street'][x] = df['address.street'].str.split('-').str[0][x] + '--' \\\n",
    "                                      + df['address.street'].str.split('-').str[1][x]\n",
    "\n",
    "        if ',' in (df['address.street'][x]):\n",
    "            add_part = str(df['address.postalId'][x]) + '-' + df['address.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(',').str[0][x].replace(\" \",\"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(', ').str[1][x].replace(\".\", \"\").replace(\" \", \"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + add_part\n",
    "        elif 'Adressen er ikke tilgængelig' in (df['address.street'][x]):\n",
    "            url = 'Utilgængelig'\n",
    "        else:\n",
    "            add_part = str(df['address.postalId'][x]) + '-' + df['address.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                       + df['address.street'].str.split(',').str[0][x].replace(\" \",\"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + add_part\n",
    "\n",
    "        if '-lejl-' in url:\n",
    "            url = url.replace('-lejl-','-')\n",
    "\n",
    "        df['dingeo_link'][x] = url\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_geolinks2(df):\n",
    "    df[\"dingeo_link\"] = \"\"\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if '-' in (df['property.address'][x]):\n",
    "            df['property.address'][x] = df['property.address'].str.split('-').str[0][x] + '--' \\\n",
    "                                        + df['property.address'].str.split('-').str[1][x]\n",
    "\n",
    "        if ',' in (df['property.address'][x]):\n",
    "            ad_part = str(df['property.postal'][x]) + '-' + df['property.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(',').str[0][x].replace(\" \",\"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(', ').str[1][x].replace(\".\", \"\").replace(\" \", \"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + ad_part\n",
    "        elif 'Adressen er ikke tilgængelig' in (df['property.address'][x]):\n",
    "            url = 'Utilgængelig'\n",
    "        else:\n",
    "            ad_part = str(df['property.postal'][x]) + '-' + df['property.city'][x].replace(\" \", \"-\") + '/' \\\n",
    "                      + df['property.address'].str.split(',').str[0][x].replace(\" \",\"-\")\n",
    "            url = 'https://www.dingeo.dk/adresse/' + ad_part\n",
    "\n",
    "        if '-lejl-' in url:\n",
    "            url = url.replace('-lejl-','-')\n",
    "\n",
    "        df['dingeo_link'][x] = url\n",
    "\n",
    "    return df\n",
    "\n",
    "#### Scrape information for each individual address on DinGeo.dk\n",
    "\n",
    "def dingeo_page(url):\n",
    "    url = url\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # Dictionary\n",
    "    data = {}\n",
    "    data['dingeo_link'] = url\n",
    "    try:\n",
    "        data['Radonrisiko'] = [soup.find_all(\"div\", {\"id\": 'radon'})[0].find_all(\"strong\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if 'ikke registreret trafikstøj' in soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].get_text():\n",
    "        data['Støjmåling'] = ['Ingen trafikstøj']\n",
    "    elif 'mangler desværre at indsamle trafikstøj' in soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].get_text():\n",
    "        data['Støjmåling'] = ['Mangler']\n",
    "    else:\n",
    "        data['Støjmåling'] = [soup.find_all(\"div\", {\"id\": 'trafikstoej'})[0].find_all(\"b\")[1].get_text()]\n",
    "\n",
    "    data['Oversvømmelsesrisiko_skybrud'] = [soup.find_all(\"div\", {\"id\": 'skybrud'})[0].find_all(\"b\")[0].get_text()]\n",
    "    data['Meter_over_havet'] = [soup.find_all(\"div\", {\"id\": 'stormflod'})[0].find_all(\"b\")[0].get_text()]\n",
    "\n",
    "    table_0 = pd.read_html(str(soup.find_all('table')))[0].iloc[:, 0:2]\n",
    "    table_0 = table_0.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_1 = pd.read_html(str(soup.find_all('table')))[1].iloc[:, 0:2]\n",
    "    table_1 = table_1.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_2 = pd.read_html(str(soup.find_all('table')))[2].iloc[:, 0:2]\n",
    "    table_2 = table_2.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table_3 = pd.read_html(str(soup.find_all('table')))[3:-2]\n",
    "    table_3 = pd.concat(table_3).iloc[:, 0:2]\n",
    "    table_3 = table_3.set_axis(['Tekst', 'Værdi'], axis=1, inplace=False)\n",
    "\n",
    "    table = pd.concat([table_0, table_1, table_2, table_3])\n",
    "\n",
    "    table = table.loc[table['Tekst'].isin(['Anvendelse', 'Opførselsesår', 'Ombygningsår', 'Fredning',\n",
    "                                           'Køkkenforhold', 'Antal Etager', 'Antal toiletter', 'Antal badeværelser',\n",
    "                                           'Antal værelser',\n",
    "                                           'Ydervægsmateriale', 'Tagmateriale', 'Varmeinstallation',\n",
    "                                           'Bygning, Samlet areal', 'Boligstørrelse', 'Kælder', 'Vægtet Areal'])]\n",
    "    mydict = dict(zip(table.Tekst, list(table.Værdi)))\n",
    "    data.update(mydict)\n",
    "\n",
    "    try:\n",
    "        if 'ikke finde energimærke' in soup.find_all(\"div\", {\"id\": 'energimaerke'})[0].get_text():\n",
    "            data['Energimærke'] = ['Mangler']\n",
    "        else:\n",
    "            data['Energimærke'] = [soup.find_all(\"div\", {\"id\": 'energimaerke'})[0].find_all(\"p\")[0].get_text()[-3:-2]]\n",
    "        data['Indbrudsrisiko'] = [soup.find_all(\"div\", {\"id\": 'indbrud'})[0].find_all(\"u\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if 'ikke fredet' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = [0]\n",
    "        elif 'Bygningen er Bevaringsværdig' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = re.findall(r'\\d+', str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"p\")[4]))\n",
    "        elif 'Fejl ved opslag af' in str(soup.find_all(\"div\", {\"id\": 'fbb'})[0].find_all(\"h2\")[0]):\n",
    "            data['Bevaringsværdig'] = 'Mangler' #Seems to be flaw on site, all get mangler\n",
    "        else:\n",
    "            data['Bevaringsværdig'] = 'Ukendt'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['Største_parti'] = re.findall(r'valg/(.*?)(?<!\\\\).png',\n",
    "                                           str(soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all('h2')[0]))\n",
    "        data['Valgdeltagelse'] = \\\n",
    "        re.findall(\"\\d+.\\d+\", str(soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all('p')[1]))[1]\n",
    "        data['Afstemningsområde'] = [soup.find_all(\"div\", {\"id\": 'valgdata'})[0].find_all(\"strong\")[0].get_text()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        url_vurdering = url + '/vurdering'\n",
    "        resp_vurdering = requests.get(url_vurdering)\n",
    "        soup_vurdering = BeautifulSoup(resp_vurdering.text, 'html.parser')\n",
    "        data['AVM_pris'] = \\\n",
    "        soup_vurdering.find_all(\"div\", {\"id\": 'avmnumber'})[0].get_text() #made correction\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        # Make dataframe\n",
    "    df_page = pd.DataFrame(data)\n",
    "\n",
    "    return df_page\n",
    "\n",
    "#### Collect all scraped data from DinGeo for the addresses and ad to Boligsiden-dataframes\n",
    "\n",
    "def for_threading(url):\n",
    "\n",
    "    try:\n",
    "        df_pages = dingeo_page(url)\n",
    "        # df_geo = pd.concat([df_geo, df_pages])\n",
    "        #   time.sleep(1)\n",
    "        return df_pages\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def add_dingeo(df):\n",
    "\n",
    "    url_list = df['dingeo_link'].tolist()\n",
    "\n",
    "    df_geo = pd.DataFrame()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = tqdm(executor.map(for_threading, url_list))\n",
    "\n",
    "        for result in results:\n",
    "            df_geo = pd.concat([df_geo, result])\n",
    "\n",
    "\n",
    "    df_Boligsiden_Dingeo = pd.merge(df, df_geo, how='inner', on='dingeo_link', right_index=False).drop_duplicates()\n",
    "\n",
    "    return df_Boligsiden_Dingeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions for scraping hvorlangterder.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape information for single address from hvorlangterder.dk\n",
    "\n",
    "def get_hvorlangterder(address):\n",
    "    try:\n",
    "        url = 'https://hvorlangterder.poi.viamap.net/v1/nearestpoi/?poitypes' \\\n",
    "              '=daycare,doctor,hospital,junction,metro,school,stop,strain,supermarket,train,library,pharmacy,coast' \\\n",
    "              ',forest,lake,airport,sportshall,publicbath,soccerfield,roadtrain&fromaddress=' + address \\\n",
    "              + '&mot=foot&token=eyJkcGZ4IjogImh2b3JsYW5ndGVyZGVyIiwgInByaXZzIjogInIxWjByMEYwazZCdFdxUWNPVXlrQi95N' \\\n",
    "                'lNVcEp2MlFiZ3lYZXRxNEhZNFhPLzNZclcwK0s5dz09In0.fP4JWis69HmaSg5jVHiK8nemiCu6VaMULSGGJyK4D4PkWq4iA1' \\\n",
    "                '+nSHWMaHxepKwJ83sEiy9nMNZhv7BcktRNrA'\n",
    "        resp = requests.get(url)\n",
    "        cont = resp.json()\n",
    "        df = pd.DataFrame(cont).loc[['routedmeters']]\n",
    "        df['Location'] = address\n",
    "\n",
    "        return (df)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#### Scrape data from hvorlangterder.dk for all adresses and merge with data from Boligsiden and DinGeo.dk\n",
    "    \n",
    "def add_hvorlangterder(df):\n",
    "\n",
    "\n",
    "    df_hvorlangt = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        try:\n",
    "            data = get_hvorlangterder(str(df['Location'][i]))\n",
    "            df_hvorlangt = pd.concat([df_hvorlangt, data])\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.2)\n",
    "\n",
    "\n",
    "    merged = pd.merge(df, df_hvorlangt, how='inner', on='Location', right_index=False).drop_duplicates()\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scrape data for the period from 01/01/16 until 31/12/21 and the danish municipalities København and Frederiksberg.\n",
    "\n",
    "First we get the links needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 2017 #######\n",
    "links = get_all_links_boligsiden('København', '2017-01-01', '2017-12-31')\n",
    "\n",
    "with open('links_boligsiden_K17.txt', 'w') as file:\n",
    "    file.write(str(links))\n",
    "    \n",
    "links = get_all_links_boligsiden('Frederiksberg', '2017-01-01', '2017-12-31')\n",
    "\n",
    "with open('links_boligsiden_F17.txt', 'w') as file:\n",
    "    file.write(str(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 2018 #######\n",
    "links = get_all_links_boligsiden('København', '2018-01-01', '2018-12-31')\n",
    "\n",
    "with open('links_boligsiden_K18.txt', 'w') as file:\n",
    "    file.write(str(links))\n",
    "    \n",
    "links = get_all_links_boligsiden('Frederiksberg', '2018-01-01', '2018-12-31')\n",
    "\n",
    "with open('links_boligsiden_F18.txt', 'w') as file:\n",
    "    file.write(str(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 2019 #######\n",
    "links = get_all_links_boligsiden('København', '2019-01-01', '2019-12-31')\n",
    "\n",
    "with open('links_boligsiden_K19.txt', 'w') as file:\n",
    "    file.write(str(links))\n",
    "    \n",
    "links = get_all_links_boligsiden('Frederiksberg', '2019-01-01', '2019-12-31')\n",
    "\n",
    "with open('links_boligsiden_F19.txt', 'w') as file:\n",
    "    file.write(str(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 298/298 [23:26<00:00,  4.72s/it]\n",
      "100%|███████████████████████████████████████████| 55/55 [04:07<00:00,  4.49s/it]\n"
     ]
    }
   ],
   "source": [
    "####### 2020 ########\n",
    "links = get_all_links_boligsiden('København', '2020-01-01', '2020-12-31')\n",
    "\n",
    "with open('links_boligsiden_K20.txt', 'w') as file:\n",
    "    file.write(str(links))\n",
    "    \n",
    "links = get_all_links_boligsiden('Frederiksberg', '2020-01-01', '2020-12-31')\n",
    "\n",
    "with open('links_boligsiden_F20.txt', 'w') as file:\n",
    "    file.write(str(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 268/268 [21:08<00:00,  4.73s/it]\n",
      "100%|███████████████████████████████████████████| 47/47 [03:34<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "######## 2021 ########\n",
    "links = get_all_links_boligsiden('København', '2021-01-01', '2021-12-31')\n",
    "\n",
    "with open('links_boligsiden_K21.txt', 'w') as file:\n",
    "    file.write(str(links))\n",
    "    \n",
    "links = get_all_links_boligsiden('Frederiksberg', '2021-01-01', '2021-12-31')\n",
    "\n",
    "with open('links_boligsiden_F21.txt', 'w') as file:\n",
    "    file.write(str(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load data from 'Boligsiden.dk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9618/9618 [1:51:51<00:00,  1.43it/s]\n",
      "100%|█████████████████████████████████████| 8332/8332 [1:37:40<00:00,  1.42it/s]\n",
      "100%|█████████████████████████████████████| 7752/7752 [1:27:19<00:00,  1.48it/s]\n",
      "100%|█████████████████████████████████████| 8934/8934 [1:37:30<00:00,  1.53it/s]\n",
      "100%|█████████████████████████████████████| 8028/8028 [1:23:02<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "####### Copenhagen ##########\n",
    "\n",
    "with open(\"links_boligsiden_K17.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "    \n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_K1.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_K1.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_K18.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "    \n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_K2.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_K2.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_K19.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_K3.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_K3.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_K20.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_K4.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_K4.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_K21.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_K5.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_K5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1672/1672 [15:07<00:00,  1.84it/s]\n",
      "100%|███████████████████████████████████████| 1477/1477 [13:18<00:00,  1.85it/s]\n",
      "100%|███████████████████████████████████████| 1501/1501 [13:26<00:00,  1.86it/s]\n",
      "100%|███████████████████████████████████████| 1621/1621 [14:17<00:00,  1.89it/s]\n",
      "100%|███████████████████████████████████████| 1390/1390 [12:10<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "############# Frederiksberg ####################\n",
    "\n",
    "with open(\"links_boligsiden_F17.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "    \n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_F1.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_F1.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_F18.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "    \n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_F2.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_F2.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_F19.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_F3.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_F3.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_F20.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_F4.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_F4.csv', index=False)\n",
    "\n",
    "with open(\"links_boligsiden_F21.txt\", \"r\") as file:\n",
    "    links = eval(file.readline())\n",
    "\n",
    "df1, df2 = get_data_boligsiden(links)\n",
    "df1.to_csv('boligsiden_1_F5.csv', index=False)\n",
    "df2.to_csv('boligsiden_2_F5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data from DinGeo.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mikkel/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (35,84,140,141,142,143,144,145,146,147,148,149,150,151) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/2t/qd3vvq4x70ld673ffm69bc6h0000gn/T/ipykernel_1171/2404662681.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dingeo_link'][x] = url\n",
      "/var/folders/2t/qd3vvq4x70ld673ffm69bc6h0000gn/T/ipykernel_1171/2404662681.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['address.street'][x] = df['address.street'].str.split('-').str[0][x] + '--' \\\n",
      "8801it [2:19:27,  1.05it/s]\n",
      "7954it [1:48:47,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "############ Copenhagen ##############\n",
    "\n",
    "#df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K1.csv\")\n",
    "#df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "#df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "#df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K1.csv', index=False)\n",
    "\n",
    "\n",
    "#df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K2.csv\")\n",
    "#df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "#df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "#df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K2.csv', index=False)\n",
    "\n",
    "#df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K3.csv\")\n",
    "#df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "#df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "#df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K3.csv', index=False)\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K4.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K4.csv', index=False)\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_K5.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_K5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/qd3vvq4x70ld673ffm69bc6h0000gn/T/ipykernel_1171/2404662681.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dingeo_link'][x] = url\n",
      "1654it [20:17,  1.36it/s]\n",
      "/var/folders/2t/qd3vvq4x70ld673ffm69bc6h0000gn/T/ipykernel_1171/2404662681.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['address.street'][x] = df['address.street'].str.split('-').str[0][x] + '--' \\\n",
      "1452it [16:41,  1.45it/s]\n",
      "1485it [16:59,  1.46it/s]\n",
      "1607it [18:06,  1.48it/s]\n",
      "1380it [15:37,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "########## Frederiksberg #############\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_F1.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_F1.csv', index=False)\n",
    "\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_F2.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_F2.csv', index=False)\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_F3.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_F3.csv', index=False)\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_F4.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_F4.csv', index=False)\n",
    "\n",
    "df_Boligsiden1 = pd.read_csv(\"boligsiden_1_F5.csv\")\n",
    "df_Boligsiden_Geo1 = get_geolinks1(df_Boligsiden1)\n",
    "df_Boligsiden_Dingeo1 = add_dingeo(df_Boligsiden_Geo1)\n",
    "\n",
    "df_Boligsiden_Dingeo1.to_csv('boligsiden_dingeo_1_F5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get the Hvorlangterder.dk data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mikkel/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (35,84,134,141,142,143,144,145,146,147,148,149,150,151,152) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "100%|█████████████████████████████████████| 9405/9405 [2:01:10<00:00,  1.29it/s]\n",
      "/Users/Mikkel/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (84,140,141,142,144,145,148,149,151) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "100%|█████████████████████████████████████| 8151/8151 [1:54:07<00:00,  1.19it/s]\n",
      "100%|█████████████████████████████████████| 7615/7615 [1:59:05<00:00,  1.07it/s]\n",
      "100%|█████████████████████████████████████| 8775/8775 [2:34:34<00:00,  1.06s/it]\n",
      "100%|█████████████████████████████████████| 7932/7932 [2:12:09<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "############ Copenhagen ##############\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K1.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K1.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K2.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K2.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K3.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K3.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K4.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K4.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_K5.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_K5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1640/1640 [39:33<00:00,  1.45s/it]\n",
      "100%|███████████████████████████████████████| 1449/1449 [19:46<00:00,  1.22it/s]\n",
      "100%|███████████████████████████████████████| 1479/1479 [14:39<00:00,  1.68it/s]\n",
      "100%|███████████████████████████████████████| 1602/1602 [14:18<00:00,  1.87it/s]\n",
      "100%|███████████████████████████████████████| 1376/1376 [10:59<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "############### Frederiksberg #################\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_F1.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_F1.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_F2.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_F2.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_F3.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_F3.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_F4.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_F4.csv', index=False)\n",
    "\n",
    "\n",
    "geo_bolig1 = pd.read_csv(\"boligsiden_dingeo_1_F5.csv\")\n",
    "geo_bolig1['Location'] = geo_bolig1['address.street'].str.split(',').str[0] + ', ' \\\n",
    "+ geo_bolig1['address.postalId'].astype(str)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1 = add_hvorlangterder(geo_bolig1)\n",
    "df_Boligsiden_Dingeo_Hvorlangterder1.to_csv('bdh_1_F5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to piece everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mikkel/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (84,134,141,142,143,144,145,146,147,148,149,150,151,152) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/Mikkel/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (84,140,141,142,143,144,145,146,147,148,149,150,151) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "bdh_1_K1 = pd.read_csv(\"bdh_1_K1.csv\")\n",
    "bdh_1_K2 = pd.read_csv(\"bdh_1_K2.csv\")\n",
    "bdh_1_K3 = pd.read_csv(\"bdh_1_K3.csv\")\n",
    "bdh_1_K4 = pd.read_csv(\"bdh_1_K4.csv\")\n",
    "bdh_1_K5 = pd.read_csv(\"bdh_1_K5.csv\")\n",
    "\n",
    "bdh_1_F1 = pd.read_csv(\"bdh_1_F1.csv\")\n",
    "bdh_1_F2 = pd.read_csv(\"bdh_1_F2.csv\")\n",
    "bdh_1_F3 = pd.read_csv(\"bdh_1_F3.csv\")\n",
    "bdh_1_F4 = pd.read_csv(\"bdh_1_F4.csv\")\n",
    "bdh_1_F5 = pd.read_csv(\"bdh_1_F5.csv\")\n",
    "\n",
    "raw_data_1 = pd.concat([bdh_1_K1, bdh_1_K2, bdh_1_K3, bdh_1_K4, bdh_1_K5,\n",
    "                        bdh_1_F1, bdh_1_F2, bdh_1_F3, bdh_1_F4, bdh_1_F5], sort=False)\n",
    "\n",
    "raw_data_1.to_csv('raw_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
